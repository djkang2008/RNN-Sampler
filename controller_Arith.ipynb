{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN based OPERATOR Sampler  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Project] Select optimal combination of arithmetic operators to solve a simple equation   \n",
    "[Base] Enas-Pytorch  \n",
    "[Date] 2018/08/27, by funmv   \n",
    "[Path] D:\\PYTORCH_CASES\\NEW_TRIAL\\ENAS-pytorch   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import utils\n",
    "import config\n",
    "from torch.autograd import Variable\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based Controller\n",
    "# RNN structure consists of one of LSTMCell and a few Linear layers  \n",
    "class Controller(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        torch.nn.Module.__init__(self)\n",
    "        self.args = args\n",
    "\n",
    "        self.num_tokens = [len(args.shared_rnn_activations)] #['*','-','*','/']\n",
    "            \n",
    "        #### the number of token per timestep and length of timestep ##########\n",
    "        self.num_tokens = [4,4,4,4,4] \n",
    "        self.args.num_blocks = 5\n",
    "        \n",
    "        # Tuning parameters: affect convergency\n",
    "        #self.args.controller_hid = 200\n",
    "        #self.args.entropy_mode = 'regularizer'\n",
    "        \n",
    "        \n",
    "        self.func_names = args.shared_rnn_activations\n",
    "\n",
    "        num_total_tokens = sum(self.num_tokens) #130->20\n",
    "\n",
    "        self.encoder = torch.nn.Embedding(num_total_tokens, #130->20\n",
    "                                          args.controller_hid) #100\n",
    "        # ONE of LSTMCell\n",
    "        self.lstm = torch.nn.LSTMCell(args.controller_hid, args.controller_hid) #100,100\n",
    "\n",
    "        # Controller RNN consists of ONE LSTMCell with size (100,100).\n",
    "        # The single LSTMCell handles timestep input repeatly. \n",
    "        # At each timestep, output of LSTMCell is used as sampler throughout DENSE(linear) layer. \n",
    "        # Most of parameters in NET are concentrated in DENSE layer\n",
    "        self.decoders = []\n",
    "        for idx, size in enumerate(self.num_tokens):\n",
    "            decoder = torch.nn.Linear(args.controller_hid, size)\n",
    "            self.decoders.append(decoder)\n",
    "\n",
    "        self._decoders = torch.nn.ModuleList(self.decoders)\n",
    "\n",
    "        self.reset_parameters() # Initialize all parameters bewteen -0.1 ~ 0.1\n",
    "        self.static_init_hidden = utils.keydefaultdict(self.init_hidden)\n",
    "\n",
    "        def _get_default_hidden(key):\n",
    "            return utils.get_variable(\n",
    "                torch.zeros(key, self.args.controller_hid), #1,100\n",
    "                self.args.cuda, #True\n",
    "                requires_grad=False)\n",
    "\n",
    "        self.static_inputs = utils.keydefaultdict(_get_default_hidden)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init_range = 0.1\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-init_range, init_range)\n",
    "        for decoder in self.decoders:\n",
    "            decoder.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self,  # pylint:disable=arguments-differ\n",
    "                inputs,\n",
    "                hidden,\n",
    "                block_idx,\n",
    "                is_embed):\n",
    "        if not is_embed:   # a constant value for input after second timestep of LSTMCell\n",
    "            embed = self.encoder(inputs) # nn.Embeding(130,100) changes the constant to (1,100) tensor\n",
    "        else:\n",
    "            embed = inputs # (1,100) tensor for first timestep input of LSTMMCell\n",
    "\n",
    "        hx, cx = self.lstm(embed, hidden) #hx(1,100),cx(1,100),embed(1,100),hidden(2,1,100)\n",
    "        logits = self.decoders[block_idx](hx)\n",
    "\n",
    "        logits /= self.args.softmax_temperature #logits(1,4)\n",
    "\n",
    "        # exploration\n",
    "        if self.args.mode == 'train':\n",
    "            logits = (self.args.tanh_c*F.tanh(logits)) #2.5*..\n",
    "\n",
    "        return logits, (hx, cx)\n",
    "\n",
    "    def sample(self, batch_size=1, with_details=False, save_dir=None):\n",
    "        \"\"\"Samples a set of `args.num_blocks` many computational nodes from the\n",
    "        controller, where each node is made up of an activation function, and\n",
    "        each node except the last also includes a previous node.\n",
    "        \"\"\"\n",
    "        if batch_size < 1:\n",
    "            raise Exception(f'Wrong batch_size: {batch_size} < 1')\n",
    "\n",
    "        # [B, L, H]\n",
    "        inputs = self.static_inputs[batch_size] #(1,100)\n",
    "        hidden = self.static_init_hidden[batch_size] #(2,1,100)\n",
    "\n",
    "        activations = []\n",
    "        entropies = []\n",
    "        log_probs = []\n",
    "        prev_nodes = []\n",
    "        \n",
    "        # NOTE(brendan): The RNN controller alternately outputs an activation,\n",
    "        # followed by a previous node, for each block except the last one,\n",
    "        # which only gets an activation function. The last node is the output\n",
    "        # node, and its previous node is the average of all leaf nodes.\n",
    "        \n",
    "        for block_idx in range(self.args.num_blocks): \n",
    "            logits, hidden = self.forward(inputs,\n",
    "                                          hidden,\n",
    "                                          block_idx,\n",
    "                                          is_embed=(block_idx == 0))\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1) #(1,4)\n",
    "            log_prob = F.log_softmax(logits, dim=-1) #(1,4)\n",
    "            # TODO(brendan): .mean() for entropy?\n",
    "            entropy = -(log_prob * probs).sum(1, keepdim=False) #(1,)\n",
    "\n",
    "            action = probs.multinomial(num_samples=1).data # a constant value\n",
    "            selected_log_prob = log_prob.gather(  # prob. corresponding to the constant action\n",
    "                1, utils.get_variable(action, requires_grad=False))\n",
    "\n",
    "            # TODO(brendan): why the [:, 0] here? Should it be .squeeze(), or\n",
    "            # .view()? Same below with `action`.\n",
    "            entropies.append(entropy) # save for return\n",
    "            log_probs.append(selected_log_prob[:, 0])\n",
    "\n",
    "            # action selected is used as next timestep input \n",
    "            inputs = utils.get_variable(action[:,0], requires_grad=False)\n",
    "            activations.append(action[:, 0])\n",
    "            \n",
    "            \n",
    "        activations = torch.stack(activations).transpose(0, 1)  \n",
    "\n",
    "        return torch.cat(log_probs), torch.cat(entropies), activations\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        zeros = torch.zeros(batch_size, self.args.controller_hid)\n",
    "        return (utils.get_variable(zeros, self.args.cuda, requires_grad=False),\n",
    "                utils.get_variable(zeros.clone(), self.args.cuda, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-30 16:51:13,792:INFO::Unparsed args: ['-f', 'C:\\\\Users\\\\DJKang\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-02bdfeba-1c94-4475-867e-18131e58c995.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activation_regularization=False, activation_regularization_amount=2.0, batch_size=64, cnn_hid=64, controller_grad_clip=0, controller_hid=100, controller_lr=0.00035, controller_lr_cosine=False, controller_lr_max=0.05, controller_lr_min=0.001, controller_max_step=2000, controller_optim='adam', cuda=True, data_dir='data', dataset='ptb', derive_num_sample=100, discount=1.0, ema_baseline_decay=0.95, entropy_coeff=0.0001, entropy_mode='reward', load_path='', log_dir='logs', log_level='INFO', log_step=50, max_epoch=150, max_save_num=4, mode='train', network_type='rnn', norm_stabilizer_fixed_point=5.0, norm_stabilizer_regularization=False, norm_stabilizer_regularization_amount=1.0, num_blocks=12, num_gpu=1, ppl_square=False, random_seed=12345, reward_c=80, save_epoch=4, shared_cnn_types=['3x3', '5x5', 'sep 3x3', 'sep 5x5', 'max 3x3', 'max 5x5'], shared_decay=0.96, shared_decay_after=15, shared_dropout=0.4, shared_dropoute=0.1, shared_dropouti=0.65, shared_embed=1000, shared_grad_clip=0.25, shared_hid=1000, shared_initial_step=0, shared_l2_reg=1e-07, shared_lr=20.0, shared_max_step=400, shared_num_sample=1, shared_optim='sgd', shared_rnn_activations=['tanh', 'ReLU', 'identity', 'sigmoid'], shared_rnn_max_length=35, shared_wdrop=0.5, softmax_temperature=5.0, tanh_c=2.5, temporal_activation_regularization=False, temporal_activation_regularization_amount=1.0, test_batch_size=1, tie_weights=True, use_tensorboard=True)\n"
     ]
    }
   ],
   "source": [
    "args, unparsed = config.get_args()\n",
    "print (args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Controller(\n",
       "  (encoder): Embedding(20, 100)\n",
       "  (lstm): LSTMCell(100, 100)\n",
       "  (_decoders): ModuleList(\n",
       "    (0): Linear(in_features=100, out_features=4, bias=True)\n",
       "    (1): Linear(in_features=100, out_features=4, bias=True)\n",
       "    (2): Linear(in_features=100, out_features=4, bias=True)\n",
       "    (3): Linear(in_features=100, out_features=4, bias=True)\n",
       "    (4): Linear(in_features=100, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Controller and their parameters\n",
    "controller = Controller(args)\n",
    "controller.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that controller.sample works \n",
    "log_probs, entropies, activations = controller.sample(with_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3865, -1.3886, -1.3727, -1.3893, -1.3918], device='cuda:0')\n",
      "tensor([ 1.3863,  1.3863,  1.3863,  1.3863,  1.3863], device='cuda:0')\n",
      "activations= tensor([[ 3,  0,  1,  2,  3]], device='cuda:0')\n",
      "[3, 0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print (log_probs)\n",
    "print (entropies)\n",
    "print ('activations= {}'.format(activations)) #12, ['tanh', 'RELU', 'Identity', 'Sigmoid']\n",
    "print (activations.data.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer setup \n",
    "controller_optim = torch.optim.Adam(controller.parameters(), lr=args.controller_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '+', 1: '-', 2: '/', 3: '*'} <class 'dict'>\n",
      "+\n",
      "[3, 0, 1, 2, 3]\n",
      "0.004630918733533246\n"
     ]
    }
   ],
   "source": [
    "# Define four arithmetic operators \n",
    "opdic = {0:'+', 1:'-', 2:'/', 3:'*'}\n",
    "print (opdic, type(opdic))\n",
    "print (opdic[0])\n",
    "\n",
    "\n",
    "# Define equation \n",
    "# Search the optimal combination of five sequential operators to sovlve equation\n",
    "# The result of eq closes to zero, then reward is good !!!\n",
    "def evaloper(op):\n",
    "    ''' op: operator list sampled by controller\n",
    "        return: reward increases when the equation closes to zero \n",
    "    '''\n",
    "    stri ='-4.0{}0.3{}(-4.0){}(-0.7){}2.0{}(-0.5)'.format(opdic[op[0]],opdic[op[1]],opdic[op[2]],opdic[op[3]],opdic[op[4]])\n",
    "    equa = eval(stri)\n",
    "    resu = math.exp(-math.fabs(equa)) # reward \n",
    "    #print (stri, equa, resu)\n",
    "    return resu\n",
    "\n",
    "# Test for a random five operators \n",
    "print (activations.data.tolist()[0])\n",
    "print (evaloper(activations.data.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0') 0.056617985941811166\n",
      "tensor(0.1517, device='cuda:0') 0.15244485102653757\n",
      "tensor(-1.3019, device='cuda:0') 0.018076816826495798\n",
      "tensor(1.00000e-02 *\n",
      "       9.8138, device='cuda:0') 0.9488543210558013\n",
      "tensor(1.00000e-02 *\n",
      "       4.1340, device='cuda:0') 0.898397321348071\n",
      "tensor(0.6101, device='cuda:0') 0.9999999999999998\n",
      "tensor(1.00000e-02 *\n",
      "       1.1036, device='cuda:0') 0.9488543210558013\n",
      "tensor(1.00000e-03 *\n",
      "       5.1819, device='cuda:0') 0.9488543210558013\n",
      "tensor(1.00000e-03 *\n",
      "       6.8021, device='cuda:0') 0.9488543210558013\n",
      "tensor(1.00000e-03 *\n",
      "       3.3121, device='cuda:0') 0.9488543210558013\n",
      "tensor(-0.4958, device='cuda:0') 0.8105842459701871\n",
      "tensor(1.00000e-03 *\n",
      "       2.5662, device='cuda:0') 0.9488543210558013\n",
      "tensor(-5.3906, device='cuda:0') 0.07243975703425146\n",
      "tensor(1.00000e-03 *\n",
      "       9.2837, device='cuda:0') 0.9488543210558013\n",
      "tensor(1.00000e-03 *\n",
      "       3.4676, device='cuda:0') 0.9488543210558013\n",
      "tensor(1.00000e-03 *\n",
      "       1.1435, device='cuda:0') 0.9488543210558013\n",
      "tensor(1.00000e-03 *\n",
      "       7.8517, device='cuda:0') 0.9488543210558013\n",
      "tensor(1.00000e-03 *\n",
      "       4.3565, device='cuda:0') 0.9488543210558013\n",
      "tensor(1.00000e-03 *\n",
      "       4.0711, device='cuda:0') 0.9488543210558013\n",
      "tensor(1.00000e-03 *\n",
      "       3.7470, device='cuda:0') 0.9488543210558013\n"
     ]
    }
   ],
   "source": [
    "# Traing RNN   \n",
    "baseline = None\n",
    "controller.reset_parameters()\n",
    "\n",
    "for step in range(10000):\n",
    "    \n",
    "    log_probs, entropies, activations = controller.sample(with_details=True)\n",
    "\n",
    "    rewards = evaloper(activations.data.tolist()[0]) \n",
    "\n",
    "    # moving average baseline\n",
    "    if baseline is None:\n",
    "        baseline = rewards\n",
    "    else:\n",
    "        decay = args.ema_baseline_decay #0.95\n",
    "        baseline = decay * baseline + (1 - decay) * rewards\n",
    "\n",
    "    adv = rewards - baseline\n",
    "#     print (adv, rewards, baseline, type(adv))\n",
    "#     print (log_probs, log_probs*Variable(torch.Tensor([adv]).cuda()) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # policy loss\n",
    "    loss = -log_probs * Variable(torch.Tensor([adv]).cuda(), requires_grad=False)\n",
    "    if controller.args.entropy_mode == 'regularizer':\n",
    "        loss -= args.entropy_coeff * entropies\n",
    "\n",
    "    loss = loss.sum()  # or loss.mean()\n",
    "    if step % 500 == 0:\n",
    "        print (loss, rewards)\n",
    "\n",
    "    # update\n",
    "    controller_optim.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    controller_optim.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 3, 2, 3] -4.0*0.3/(-4.0)*(-0.7)/2.0*(-0.5) = 0.0525\n"
     ]
    }
   ],
   "source": [
    "# Verify the result \n",
    "# The combination of five operators gives a value close to zero\n",
    "op = activations.data.tolist()[0]\n",
    "stri = '-4.0{}0.3{}(-4.0){}(-0.7){}2.0{}(-0.5)'.format(opdic[op[0]],opdic[op[1]],opdic[op[2]],opdic[op[3]],opdic[op[4]])\n",
    "print (op,stri,'=',eval(stri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
